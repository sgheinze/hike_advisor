{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black' size=6><u> **Navigate AllTrails.com and extract all CA hikes** </u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory code for a <font color='green'>**single webpage**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(' — incognito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path='/Users/stefanheinze/Desktop/chromedriver', options=option)\n",
    "browser.get('https://www.alltrails.com/us/california')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_more_button = browser.find_element_by_xpath('//*[@id=\"load_more\"]/a/div/h3')\n",
    "centered_button = browser.execute_script(\"arguments[0].scrollIntoView()\", load_more_button)\n",
    "time.sleep(0.25)\n",
    "load_more_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 20 seconds for page to load\n",
    "timeout = 20\n",
    "try:\n",
    "    WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"load_more\"]/a/div/h3')))\n",
    "except TimeoutException:\n",
    "    print('Timed out waiting for page to load')\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"load_more\"]/a/div/h3')))\n",
    "browser.find_element_by_xpath('//*[@id=\"load_more\"]/a/div/h3').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(browser.page_source)\n",
    "html = soup.find_all('h3', class_='name xlate-none short')\n",
    "hike_names = [ hike.text for hike in html ]\n",
    "hrefs = [ hike.find('a')['href'] for hike in html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the code to functions and automatically  <font color='green'> **extract all hike urls in CA**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_list = []\n",
    "with open('agent_list.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for r in reader:\n",
    "        for agent in r:\n",
    "            agent_list.append(agent)\n",
    "\n",
    "def open_chrome():\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(' — incognito')\n",
    "    user_agent = random.choice(agent_list)\n",
    "    option.add_argument(f'user-agent={user_agent}')\n",
    "    browser = webdriver.Chrome(executable_path='/Users/stefanheinze/Desktop/chromedriver', options=option)\n",
    "    \n",
    "    return browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_display(current_value, iterable, start_time):\n",
    "    current_index = iterable.index(current_value)\n",
    "    current_time = datetime.now()\n",
    "    delta_time = (current_time - start_time).total_seconds()\n",
    "    clear_output(wait=True)\n",
    "    print(str(current_index) + '/' + str(len(iterable)) + ' (' + str(round(current_index/len(iterable)*100, 1)) + '%)')  # use display(f) if you encounter performance issues\n",
    "    print(str(int(delta_time)) + ' seconds have elapsed.')\n",
    "    print('Expected time remaining is ' + str(int((len(iterable) - current_index) * (delta_time / current_index))) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_hikes():\n",
    "    '''Loads the California section of AllTrails.com and clicks through\n",
    "    the \"Load More Hikes\" button until it no longer appears (i.e. all\n",
    "    hikes are loaded.\n",
    "    \n",
    "    Returns HTML soup.'''\n",
    "    \n",
    "    browser = open_chrome()\n",
    "    browser.get('https://www.alltrails.com/us/california')\n",
    "    \n",
    "    # Wait 20 seconds for page to load\n",
    "    timeout = 20\n",
    "    try:\n",
    "        WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"load_more\"]/a/div/h3')))\n",
    "    except TimeoutException:\n",
    "        print('Timed out waiting for page to load')\n",
    "        browser.quit()\n",
    "    \n",
    "    count = 0\n",
    "    start_time = datetime.now()\n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"load_more\"]/a/div/h3')))\n",
    "            load_more_button = browser.find_element_by_xpath('//*[@id=\"load_more\"]/a/div/h3')\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView()\", load_more_button)\n",
    "            time.sleep(1)\n",
    "            load_more_button.click()\n",
    "            count += 24\n",
    "            progress_display(count, range(8200), start_time=start_time)\n",
    "            time.sleep(random.uniform(0,2))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "    soup = bs4.BeautifulSoup(browser.page_source)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hike_urls(soup):\n",
    "    '''Extracts hike links from the fully loaded AllTrails.com California webpage.\n",
    "    \n",
    "    Returns hike urls in a list.'''\n",
    "    \n",
    "    hikes_html = soup.find_all('h3', class_='name xlate-none short')\n",
    "    #hike_names = [ hike.text for hike in hikes_html ]\n",
    "    hike_hrefs = [ hike.find('a')['href'] for hike in hikes_html ]\n",
    "    hike_urls = [ 'https://www.alltrails.com' + href for href in hike_hrefs ]\n",
    "    \n",
    "    return hike_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(listname, filename):\n",
    "    '''Saves list to csv, with each item in the same row separated by a comma.'''\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for item in listname:\n",
    "            writer.writerow([str(item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8184/8200 (99.8%)\n",
      "3629 seconds have elapsed.\n",
      "Expected time remaining is 7 seconds.\n",
      "8208 is not in range\n"
     ]
    }
   ],
   "source": [
    "soup = load_all_hikes()\n",
    "hike_urls = extract_hike_urls(soup)\n",
    "save_list(hike_urls, 'hike_urls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from hike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single hike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path='/Users/stefanheinze/Desktop/chromedriver', options=option)\n",
    "browser.get('https://www.alltrails.com/trail/us/california/potato-chip-rock-via-mt-woodson-trail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 20 seconds for page to load\n",
    "timeout = 20\n",
    "try:\n",
    "    WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"main-photo\"]/div[1]')))\n",
    "except TimeoutException:\n",
    "    print('Timed out waiting for page to load')\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"load_more\"]/div[2]/h3')))\n",
    "browser.find_element_by_xpath('//*[@id=\"load_more\"]/div[2]/h3').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(browser.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "hike_name = soup.find('title').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty = soup.find('div', id='difficulty-and-rating').find('span').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_elev_type = soup.find_all('div', class_='detail-data')\n",
    "distance = dist_elev_type[0].text\n",
    "elevation = dist_elev_type[1].text\n",
    "hike_type = dist_elev_type[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data = soup.find_all('span', class_='big rounded active')\n",
    "tags = []\n",
    "for tag in tag_data:\n",
    "    tags.append(tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = soup.find('p', class_='xlate-google').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_info = soup.find_all('span', itemprop='author')\n",
    "user_name = [ item.text for item in reviewer_info ]\n",
    "user_href = []\n",
    "for item in reviewer_info:\n",
    "    try:\n",
    "        user_href.append(item.parent['href'])\n",
    "    except:\n",
    "        user_href.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_html = soup.find_all('meta', itemprop='ratingValue')\n",
    "user_rating = [ int(item['content']) for item in reviews_html[1:] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_html = soup.find_all('meta', itemprop='datePublished')\n",
    "user_date = [ datetime.strptime(item['content'], '%Y-%m-%d') for item in publish_html ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_html = soup.find_all('p', itemprop='reviewBody')\n",
    "user_text = [ item.text for item in text_html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted items:\n",
    " - hike_name\n",
    " - hike_difficulty\n",
    " - hike_distance\n",
    " - hike_elevation\n",
    " - hike_type\n",
    " - hike_tags\n",
    " - hike_description\n",
    " - user_names\n",
    " - user_hrefs\n",
    " - user_texts\n",
    " - user_ratings\n",
    " - user_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate all hikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hike_page(hike_link):\n",
    "    '''\n",
    "    Loads webpage for a given hike on AllTrails.com and loads all reviews by \n",
    "    clicking through the \"Load More Reviews\" button.\n",
    "    \n",
    "    Returns HTML of webpage.\n",
    "    '''\n",
    "    \n",
    "    browser = open_chrome()\n",
    "    browser.get(hike_link)\n",
    "\n",
    "    # Wait 20 seconds for page to load\n",
    "    timeout = 20\n",
    "    try:\n",
    "        WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"main-photo\"]/div[1]')))\n",
    "    except TimeoutException:\n",
    "        print('Timed out waiting for page to load')\n",
    "        browser.quit()\n",
    "\n",
    "    # Load all reviews\n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(browser, 1).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"load_more\"]/div[2]/h3')))\n",
    "            load_more_button = browser.find_element_by_xpath('//*[@id=\"load_more\"]/div[2]/h3')\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView()\", load_more_button)\n",
    "            time.sleep(1)\n",
    "            load_more_button.click()\n",
    "            time.sleep(random.uniform(0,1))\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    soup = bs4.BeautifulSoup(browser.page_source)\n",
    "    browser.close()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hike_data(soup):\n",
    "    '''\n",
    "    Extract the following data for a single hike on AllTrails.com:\n",
    "    \n",
    "       Variable (type)\n",
    "     - hike_name (str)\n",
    "     - hike_difficulty (str)\n",
    "     - hike_distance (str)\n",
    "     - hike_elevation (str)\n",
    "     - hike_type (str)\n",
    "     - hike_tags (list of str)\n",
    "     - hike_description (str)\n",
    "     - user_names (list of str)\n",
    "     - user_hrefs (list of str)\n",
    "     - user_texts (list of str)\n",
    "     - user_ratings (list of int)\n",
    "     - user_dates (list of datetimes)\n",
    "     \n",
    "     Returns a dict with the above keys.\n",
    "    '''\n",
    "    \n",
    "    # Get hike name\n",
    "    try:\n",
    "        hike_name = soup.find('title').text\n",
    "    except:\n",
    "        hike_name = None\n",
    "    \n",
    "    # Get hike difficulty\n",
    "    try:\n",
    "        hike_difficulty = soup.find('div', id='difficulty-and-rating').find('span').text\n",
    "    except:\n",
    "        hike_difficulty = None\n",
    "    \n",
    "    # Get hike distance, elevation, and type\n",
    "    try:\n",
    "        dist_elev_type = soup.find_all('div', class_='detail-data')\n",
    "        hike_distance = dist_elev_type[0].text\n",
    "        hike_elevation = dist_elev_type[1].text\n",
    "        hike_type = dist_elev_type[2].text\n",
    "    except:\n",
    "        hike_distance = None\n",
    "        hike_elevation = None\n",
    "        hike_type = None\n",
    "    \n",
    "    # Get tags associated with hike\n",
    "    try:\n",
    "        tag_data = soup.find_all('span', class_='big rounded active')\n",
    "        hike_tags = []\n",
    "        for tag in tag_data:\n",
    "            hike_tags.append(tag.text)\n",
    "    except:\n",
    "        hike_tags = []\n",
    "    \n",
    "    # Get hike description\n",
    "    try:\n",
    "        hike_description = soup.find('p', class_='xlate-google').text\n",
    "    except:\n",
    "        hike_description = None\n",
    "    \n",
    "    # Get user names and hrefs (which serve as unique ID)\n",
    "    try:\n",
    "        reviewer_info = soup.find_all('span', itemprop='author')\n",
    "        user_names = [ item.text for item in reviewer_info ]\n",
    "        user_hrefs = []\n",
    "        for item in reviewer_info:\n",
    "            # Some users don't have an ID\n",
    "            try:\n",
    "                user_hrefs.append(item.parent['href'])\n",
    "            except:\n",
    "                user_hrefs.append(None)\n",
    "    except:\n",
    "        user_names = None\n",
    "        user_hrefs = None\n",
    "    \n",
    "    # Get user ratings\n",
    "    try:\n",
    "        reviews_html = soup.find_all('meta', itemprop='ratingValue')\n",
    "        user_ratings = [ int(item['content']) for item in reviews_html[1:] ]\n",
    "    except:\n",
    "        user_ratings = None\n",
    "    \n",
    "    # Get user review publish dates\n",
    "    try:\n",
    "        publish_html = soup.find_all('meta', itemprop='datePublished')\n",
    "        user_dates = [ datetime.strptime(item['content'], '%Y-%m-%d') for item in publish_html ]\n",
    "    except:\n",
    "        user_dates = None\n",
    "    \n",
    "    # Get user review text\n",
    "    try:\n",
    "        text_html = soup.find_all('p', itemprop='reviewBody')\n",
    "        user_texts = [ item.text for item in text_html ]\n",
    "    except: user_texts = None\n",
    "    \n",
    "    # Create dictionary with all scraped information\n",
    "    hike_info_dict = {}\n",
    "    hike_info_dict['hike_name'] = hike_name\n",
    "    hike_info_dict['hike_difficulty'] = hike_difficulty\n",
    "    hike_info_dict['hike_distance'] = hike_distance\n",
    "    hike_info_dict['hike_elevation'] = hike_elevation\n",
    "    hike_info_dict['hike_type'] = hike_type\n",
    "    hike_info_dict['hike_tags'] = hike_tags\n",
    "    hike_info_dict['hike_description'] = hike_description\n",
    "    hike_info_dict['user_names'] = user_names\n",
    "    hike_info_dict['user_hrefs'] = user_hrefs\n",
    "    hike_info_dict['user_texts'] = user_texts\n",
    "    hike_info_dict['user_ratings'] = user_ratings\n",
    "    hike_info_dict['user_dates'] = user_dates\n",
    "\n",
    "    return hike_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(hike_info_dict, filename):\n",
    "    '''\n",
    "    Saves scraped dictionary (hike_info_dict) to a csv file (filename) using pandas\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        hike_df = pd.read_csv(filename, index_col=False)\n",
    "        hike_df = hike_df.append(pd.DataFrame([hike_info_dict], columns=hike_info_dict.keys()))\n",
    "        hike_df.to_csv(filename, index=False)\n",
    "    except FileNotFoundError:\n",
    "        hike_df = pd.DataFrame([hike_info_dict], columns=hike_info_dict.keys())\n",
    "        hike_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_display(current_value, iterable, start_time):\n",
    "    current_index = iterable.index(current_value)\n",
    "    current_time = datetime.now()\n",
    "    delta_time = (current_time - start_time).total_seconds()\n",
    "    clear_output(wait=True)\n",
    "    print(str(current_index) + '/' + str(len(iterable)) + ' (' + str(round(current_index/len(iterable)*100, 1)) + '%)')  # use display(f) if you encounter performance issues\n",
    "    print(str(int(delta_time)) + ' seconds have elapsed.')\n",
    "    print('Expected time remaining is ' + str(int((len(iterable) - current_index) * (delta_time / current_index))) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hikes_data(hike_urls, filename):\n",
    "    '''\n",
    "    Extract data for all hikes with urls in hike_links. Saves data to filename after\n",
    "    every successful webpage scrape.\n",
    "    '''\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    for hike_url in hike_urls:\n",
    "        try:\n",
    "            soup = load_hike_page(hike_url)\n",
    "            hike_info_dict = extract_hike_data(soup)\n",
    "            save_progress(hike_info_dict, filename)\n",
    "            progress_display(hike_url, hike_urls, start_time)\n",
    "        except:\n",
    "            print('Error! Last hike saved was ' + hike_info_dict['hike_name'] + '!')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_hikes_data(hike_links, 'hike_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make agent list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path='/Users/stefanheinze/Desktop/chromedriver', options=option)\n",
    "browser.get('http://useragentstring.com/pages/useragentstring.php?name=All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(browser.page_source)\n",
    "user_agents = [ agent.text for agent in soup.find_all('a') ][5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list(user_agents, 'agent_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hike_link = 'https://www.alltrails.com/trail/us/california/san-antonio-ski-hut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = load_hike_page(hike_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_single_hike_data() got an unexpected keyword argument 'index_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-c6b9f12d5947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhike_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_single_hike_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: extract_single_hike_data() got an unexpected keyword argument 'index_col'"
     ]
    }
   ],
   "source": [
    "hike_data_dict = extract_single_hike_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hike_name', 'hike_difficulty', 'hike_distance', 'hike_elevation', 'hike_type', 'hike_tags', 'hike_description', 'user_names', 'user_hrefs', 'user_texts', 'user_ratings', 'user_dates'])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hike_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  San Antonio Ski Hut - California\\n | AllTrails'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hike_data_dict['hike_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = extract_single_hike_data(soup)\n",
    "test_df = pd.DataFrame(test['user_ratings'], columns=['user_ratings'])\n",
    "\n",
    "ratings = test_df['user_ratings'].value_counts()\n",
    "for index in range(6):\n",
    "    if index not in ratings:\n",
    "        ratings[index] = 0\n",
    "\n",
    "ratings.sort_index().plot.bar(rot=0);\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Ratings on Potato Chip Rock Hike')\n",
    "plt.savefig('ExampleDistRatings.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='green'><u>Many Tests</u></font>  \n",
    "This is a **test!**  \n",
    "This is *another* test!  \n",
    "And ***another!***\n",
    "\n",
    "Question: This markdown is a...?  \n",
    "(a) test  \n",
    "(b) *test*  \n",
    "(c) **test**  \n",
    "(d) ***test***\n",
    "\n",
    "It's obviously an ~~exam~~\n",
    "\n",
    "...one last test...\n",
    "\n",
    "`Test == Test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
