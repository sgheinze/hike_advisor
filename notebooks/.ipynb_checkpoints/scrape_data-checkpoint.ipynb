{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from IPython.display import clear_output, display\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black' size=6><u> **Navigate AllTrails.com and extract all CA hikes** </u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement code to  <font color='green'> **extract all hike urls in CA**</font> from AllTrails.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_list = []\n",
    "with open('agent_list.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for r in reader:\n",
    "        for agent in r:\n",
    "            agent_list.append(agent)\n",
    "\n",
    "def open_chrome():\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(' â€” incognito')\n",
    "    user_agent = random.choice(agent_list)\n",
    "    option.add_argument(f'user-agent={user_agent}')\n",
    "    browser = webdriver.Chrome(executable_path='/Users/stefanheinze/Desktop/chromedriver', options=option)\n",
    "    \n",
    "    return browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_display(current_value, iterable, start_time):\n",
    "    current_index = iterable.index(current_value)\n",
    "    current_time = datetime.now()\n",
    "    delta_time = (current_time - start_time).total_seconds()\n",
    "    clear_output(wait=True)\n",
    "    print(str(current_index) + '/' + str(len(iterable)) + ' (' + str(round(current_index/len(iterable)*100, 1)) + '%)')  # use display(f) if you encounter performance issues\n",
    "    print(str(int(delta_time)) + ' seconds have elapsed.')\n",
    "    print('Expected time remaining is ' + str(int((len(iterable) - current_index) * (delta_time / current_index))) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_hikes():\n",
    "    '''Loads the California section of AllTrails.com and clicks through\n",
    "    the \"Load More Hikes\" button until it no longer appears (i.e. all\n",
    "    hikes are loaded.\n",
    "    \n",
    "    Returns HTML soup.'''\n",
    "    \n",
    "    browser = open_chrome()\n",
    "    browser.get('https://www.alltrails.com/us/california')\n",
    "    \n",
    "    # Wait 20 seconds for page to load\n",
    "    timeout = 20\n",
    "    try:\n",
    "        WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"load_more\"]/a/div/h3')))\n",
    "    except TimeoutException:\n",
    "        print('Timed out waiting for page to load')\n",
    "        browser.quit()\n",
    "    \n",
    "    count = 0\n",
    "    start_time = datetime.now()\n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"load_more\"]/a/div/h3')))\n",
    "            load_more_button = browser.find_element_by_xpath('//*[@id=\"load_more\"]/a/div/h3')\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView()\", load_more_button)\n",
    "            time.sleep(1)\n",
    "            load_more_button.click()\n",
    "            count += 24\n",
    "            progress_display(count, range(8200), start_time=start_time)\n",
    "            time.sleep(random.uniform(0,2))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "    soup = bs4.BeautifulSoup(browser.page_source)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hike_urls(soup):\n",
    "    '''Extracts hike links from the fully loaded AllTrails.com California webpage.\n",
    "    \n",
    "    Returns hike urls in a list.'''\n",
    "    \n",
    "    hikes_html = soup.find_all('h3', class_='name xlate-none short')\n",
    "    #hike_names = [ hike.text for hike in hikes_html ]\n",
    "    hike_hrefs = [ hike.find('a')['href'] for hike in hikes_html ]\n",
    "    hike_urls = [ 'https://www.alltrails.com' + href for href in hike_hrefs ]\n",
    "    \n",
    "    return hike_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(listname, filename):\n",
    "    '''Saves list to csv, with each item in the same row separated by a comma.'''\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for item in listname:\n",
    "            writer.writerow([str(item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8184/8200 (99.8%)\n",
      "3629 seconds have elapsed.\n",
      "Expected time remaining is 7 seconds.\n",
      "8208 is not in range\n"
     ]
    }
   ],
   "source": [
    "soup = load_all_hikes()\n",
    "hike_urls = extract_hike_urls(soup)\n",
    "save_list(hike_urls, 'hike_urls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from hike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement code to  <font color='green'> **extract hike data from hike urls**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hike_page(hike_link):\n",
    "    '''\n",
    "    Loads webpage for a given hike on AllTrails.com and loads all reviews by \n",
    "    clicking through the \"Load More Reviews\" button.\n",
    "    \n",
    "    Returns HTML of webpage.\n",
    "    '''\n",
    "    \n",
    "    browser = open_chrome()\n",
    "    browser.get(hike_link)\n",
    "\n",
    "    # Wait 20 seconds for page to load\n",
    "    timeout = 30\n",
    "    try:\n",
    "        WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"main-photo\"]/div[1]')))\n",
    "    except TimeoutException:\n",
    "        print('Timed out waiting for page to load')\n",
    "        browser.quit()\n",
    "\n",
    "    # Load all reviews\n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(browser, 5).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"load_more\"]/div[2]/h3')))\n",
    "            load_more_button = browser.find_element_by_xpath('//*[@id=\"load_more\"]/div[2]/h3')\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView()\", load_more_button)\n",
    "            time.sleep(1)\n",
    "            load_more_button.click()\n",
    "            time.sleep(random.uniform(0,1))\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    soup = bs4.BeautifulSoup(browser.page_source)\n",
    "    browser.close()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hike_data(soup):\n",
    "    '''\n",
    "    Extract the following data for a single hike on AllTrails.com:\n",
    "    \n",
    "       Variable (type)\n",
    "     - hike_name (str)\n",
    "     - hike_difficulty (str)\n",
    "     - hike_distance (str)\n",
    "     - hike_elevation (str)\n",
    "     - hike_type (str)\n",
    "     - hike_tags (list of str)\n",
    "     - hike_description (str)\n",
    "     - user_names (list of str)\n",
    "     - user_hrefs (list of str)\n",
    "     - user_texts (list of str)\n",
    "     - user_ratings (list of int)\n",
    "     - user_dates (list of datetimes)\n",
    "     \n",
    "     Returns a dict with the above keys.\n",
    "    '''\n",
    "    \n",
    "    # Get hike name\n",
    "    try:\n",
    "        hike_name = soup.find('title').text\n",
    "    except:\n",
    "        hike_name = None\n",
    "    \n",
    "    # Get hike difficulty\n",
    "    try:\n",
    "        hike_difficulty = soup.find('div', id='difficulty-and-rating').find('span').text\n",
    "    except:\n",
    "        hike_difficulty = None\n",
    "    \n",
    "    # Get hike distance, elevation, and type\n",
    "    try:\n",
    "        dist_elev_type = soup.find_all('div', class_='detail-data')\n",
    "        hike_distance = dist_elev_type[0].text\n",
    "        hike_elevation = dist_elev_type[1].text\n",
    "        hike_type = dist_elev_type[2].text\n",
    "    except:\n",
    "        hike_distance = None\n",
    "        hike_elevation = None\n",
    "        hike_type = None\n",
    "    \n",
    "    # Get tags associated with hike\n",
    "    try:\n",
    "        tag_data = soup.find_all('span', class_='big rounded active')\n",
    "        hike_tags = []\n",
    "        for tag in tag_data:\n",
    "            hike_tags.append(tag.text)\n",
    "    except:\n",
    "        hike_tags = None\n",
    "    \n",
    "    # Get hike description\n",
    "    try:\n",
    "        hike_description = soup.find('p', class_='xlate-google').text\n",
    "    except:\n",
    "        hike_description = None\n",
    "    \n",
    "    # Get user names and hrefs (which serve as unique ID)\n",
    "    try:\n",
    "        reviewer_info = soup.find_all('span', itemprop='author')\n",
    "        user_names = [ item.text for item in reviewer_info ]\n",
    "        user_hrefs = []\n",
    "        for item in reviewer_info:\n",
    "            # Some users don't have an ID\n",
    "            try:\n",
    "                user_hrefs.append(item.parent['href'])\n",
    "            except:\n",
    "                user_hrefs.append(None)\n",
    "    except:\n",
    "        user_names = None\n",
    "        user_hrefs = None\n",
    "    \n",
    "    # Get user ratings\n",
    "    try:\n",
    "        reviews_html = soup.find_all('meta', itemprop='ratingValue')\n",
    "        user_ratings = [ int(item['content']) for item in reviews_html[1:] ]\n",
    "    except:\n",
    "        user_ratings = None\n",
    "    \n",
    "    # Get user review publish dates\n",
    "    try:\n",
    "        publish_html = soup.find_all('meta', itemprop='datePublished')\n",
    "        user_dates = [ datetime.strptime(item['content'], '%Y-%m-%d') for item in publish_html ]\n",
    "    except:\n",
    "        user_dates = None\n",
    "    \n",
    "    # Get user review text\n",
    "    try:\n",
    "        text_html = soup.find_all('p', itemprop='reviewBody')\n",
    "        user_texts = [ item.text for item in text_html ]\n",
    "    except: user_texts = None\n",
    "    \n",
    "    # Create dictionary with all scraped information\n",
    "    hike_info_dict = {}\n",
    "    hike_info_dict['hike_name'] = hike_name\n",
    "    hike_info_dict['hike_difficulty'] = hike_difficulty\n",
    "    hike_info_dict['hike_distance'] = hike_distance\n",
    "    hike_info_dict['hike_elevation'] = hike_elevation\n",
    "    hike_info_dict['hike_type'] = hike_type\n",
    "    hike_info_dict['hike_tags'] = hike_tags\n",
    "    hike_info_dict['hike_description'] = hike_description\n",
    "    hike_info_dict['user_names'] = user_names\n",
    "    hike_info_dict['user_hrefs'] = user_hrefs\n",
    "    hike_info_dict['user_texts'] = user_texts\n",
    "    hike_info_dict['user_ratings'] = user_ratings\n",
    "    hike_info_dict['user_dates'] = user_dates\n",
    "\n",
    "    return hike_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(hike_info_dict, filename):\n",
    "    '''\n",
    "    Saves scraped dictionary (hike_info_dict) to a csv file (filename) using pandas\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        hike_df = pd.read_pickle(filename)\n",
    "        hike_df = hike_df.append(pd.DataFrame([hike_info_dict], columns=hike_info_dict.keys()))\n",
    "        hike_df.to_pickle(filename)\n",
    "    except FileNotFoundError: # first write to file\n",
    "        hike_df = pd.DataFrame([hike_info_dict], columns=hike_info_dict.keys())\n",
    "        hike_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executor_func(url, filename):\n",
    "    soup = load_hike_page(url)\n",
    "    hike_info_dict = extract_hike_data(soup)\n",
    "    save_progress(hike_info_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n"
     ]
    }
   ],
   "source": [
    "with ProcessPoolExecutor(max_workers=6) as executor:\n",
    "    future_results = {executor.submit(executor_func, url=url, filename='hike_data_pickle'): url for url in hike_urls[203:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read hike_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_csv('hike_urls.csv')\n",
    "hike_urls = [ url[0] for url in urls_df.values ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make agent list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path='/Users/stefanheinze/Desktop/chromedriver', options=option)\n",
    "browser.get('http://useragentstring.com/pages/useragentstring.php?name=All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(browser.page_source)\n",
    "user_agents = [ agent.text for agent in soup.find_all('a') ][5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list(user_agents, 'agent_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='green'><u>Many Tests</u></font>  \n",
    "This is a **test!**  \n",
    "This is *another* test!  \n",
    "And ***another!***\n",
    "\n",
    "Question: This markdown is a...?  \n",
    "(a) test  \n",
    "(b) *test*  \n",
    "(c) **test**  \n",
    "(d) ***test***\n",
    "\n",
    "It's obviously an ~~exam~~\n",
    "\n",
    "...one last test...\n",
    "\n",
    "`Test == Test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "hike_data = pd.read_pickle('all_hike_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ratings = {}\n",
    "for num in range(0,1000):\n",
    "    num_ratings[num] = (hike_data['user_ratings'].apply(len) == num).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins=np.arange(1,1010,10)\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.hist(num_ratings.values(), bins=bins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = pd.Series(test['user_hrefs'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5367 hikes\n",
      "105535 unique users\n",
      "226618 text reviews.\n",
      "sparsity = 0.0400097919100818\n"
     ]
    }
   ],
   "source": [
    "print(str(test.shape[0]) + ' hikes')\n",
    "print(str(len(hrefs.unique())) + ' unique users')\n",
    "print(str((hrefs.isnull() == False).sum()) + ' text reviews.')\n",
    "\n",
    "sparsity = 226618 / (5367*105535) * 100\n",
    "print('sparsity = ' + str(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'a': {'1': 'this', '2': 'is'}, 'b': {'2': 'a', '3': 'test.'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>test.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a      b\n",
       "1  this    NaN\n",
       "2    is      a\n",
       "3   NaN  test."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
